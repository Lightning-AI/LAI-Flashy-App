import sys
import flash
from flash.core.data.utils import download_data
from {{ data_module_import_path }} import {{ data_module_class }}
from {{ task_import_path }} import {{ task_class }}
from pytorch_lightning.callbacks.progress.base import ProgressBarBase
from lightning.utilities.state import AppState

app_state = sys.argv[1]

class AppProgressBar(ProgressBarBase):

    def __init__(self):
        super().__init__()

        self._total = 0

    def disable(self):
        pass

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        super().on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)
        app_state.progress = (self._total + self.train_batch_idx) / (self.total_train_batches * self.trainer.max_epochs)

    def on_train_epoch_end(self, trainer, pl_module):
        super().on_train_epoch_end(trainer, pl_module)
        self._total += self.total_train_batches

download_data("{{ url }}", ".")

datamodule = {{ data_module_class }}.{{ data_config["target"] }}(
    {% for key, value in data_config.items() if key != "target" %}{{ key }}={% if value is string %}"{{ value }}"{% else %}{{ value }}{% endif %},{% endfor %}
    batch_size=4,
)

model = {{ task_class }}(
    {% for key, value in task_config.items() %}{{ key }}={% if value is string %}"{{ value }}"{% else %}{{ value }}{% endif %},{% endfor %}
    {% for linked_attribute in linked_attributes %}{{ linked_attribute }}=datamodule.{{ linked_attribute }},{% endfor %}
)

trainer = flash.Trainer(max_epochs=1, callbacks=[AppProgressBar()], default_root_dir="{{ root }}")
trainer.finetune(model, datamodule=datamodule, strategy="freeze")
